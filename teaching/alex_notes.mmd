Title: French schwa project to do and notes
Author: Brian W. Smith
Date: 8/13 to present
Comment: French schwa project to do and notes
CSS: bws.css

***

<div class="title">

[%title]

</div>

{{TOC}}

===

# Model evaluation and goals
- How to evaluate and diagnose the model
	- What's the baseline model?
	- What are goals? (statistical evaluation)
	
# Learning algorithm and training
- The induction mechanism has two big parts, which I'll give names
	- **Outlier Detector** Detect whether a word is an outlier
	- **Constraint Choooser** Determine which constraint(s) to index the word to
- **Outlier Detector** possibilities
	- Residuals method (requires a model)
		- For every word, compare the model predictions to the real probabilities
		- Basically, calculate residuals
		- z-score residuals and find outliers with a threshhold (e.g. Z > 2.5 SD)
		- Nice thing: works with ANY model afaik, takes into account different categories and contexts
		- Bad thing: requires a ton of calculations
	- A dumb method: univariate tails cutoff
		+ Calculate actual probability of schwa for every word
		+ If the mean rate isn't extreme (e.g. if schwa rates are closer to 50%), he probabilities will be normally distributed
		+ Read more on binomial distribution and central limit theorem to determine the actual conditions for the CLT (it's messy!)
		+ cut off words that are extreme -- these will be ones that are more categorical
		+ Nice thing: this doesn't require a model, is blindingly fast
		+ Bad thing: doesn't take into account context, will identify categorical cases as outliers, even if they obey the grammar
	- A weird adaptation of univariate tails cutoff
		+ Do the method above, but grouping together words with the same violation profile
		+ Idea: same violation profile = same phonological and morphological environment
			* Depends on constraint set, of course
			* It's more like: these are externally defined classes
			* for this data set, class is isomorphic with violation profile 
		+ All words with the same violation profile should be normally distributed around some baseline probability
		+ This actually works, I think, and is a great way to identify outliers
		+ An open question is whether this should be done as part of outlier detection in the learner, or just done to identify outliers before the fact
		+ This is very offline
		+ Excitingly, constraint induction means the word is no longer part of the same class. it's now in its own class -- since it now violates a constraint it didn't violate before
		+ A weirdness: as outliers are removed, the mean and SD changes. this is a big problem in outlier removal geneally
			* mean is very sensitive to outliers, so the mean changes when they're removed
		- some words in Racine's data simply don't have schwa -- these would need to be pruned ahead of time (idea: cut out words that don't vary)
		- Normal approximation of the binomial distribution
			+ [psu](https://newonlinecourses.science.psu.edu/stat414/node/179/)
			+ [bu](http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Probability/BS704_Probability12.html)
			+ requirements for n: np ≥ 5     and     n(1 − p) ≥ 5
			+ If p(schwa) for some category is 0.1, we need at least 50 observations to approximate normal distribution
				- we need n(1-0.1)≥ 5, n ≥ (5/.9), n ≥ 5.5
				- we need n(0.1)≥ 5, n ≥ (5/.1), n ≥ 50
- Read about outlier detection
	- ML outlier detection?
		- Lots about numerical data, hard to find proportion data
			- Weird, because this is a classifier! and ML always does classifiers
			- But: most ML outlier detection is supervised. Ugh.
		- We want ultivariate and unsupervised
	- Backward/forward model selection
		- [helpful slides on model selection: BIC vs. AIC vs. LRT](http://sct.uab.cat/estadistica/sites/sct.uab.cat.estadistica/files/uab_june_2017.pdf)
		- [overview of backward and forward selection](http://gerardnico.com/data_mining/stepwise_regression)
		- Stephanie Shih — backward model selection for exceptionality

# Outlier detection notes
- Link: [wiki article on Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance)
- Link: [towards data science article on outlier detection](http://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561)
- [Slides for paper discussing Mahalnobis distance](https://pdfs.semanticscholar.org/6712/7a1c5131218518225822067da0370bfc578f.pdf)
- Problem: we need to detect outliers in categorical data -- harder
	- Found this: [Detection of Outliers and Influential Observations in Binary Logistic Regression: An Empirical Study](https://scialert.net/fulltextmobile/?doi=jas.2011.26.35)
- Types of outliers
	- univariate vs. multivariate
		- "Univariate outliers can be found when looking at a distribution of values in a single feature space. Multivariate outliers can be found in a n-dimensional space (of n-features)."
	- point outliers "Point outliers are single data points that lay far from the rest of the distribution."
	- contextual outliers "Contextual outliers can be noise in data, such as punctuation symbols when realizing text analysis or background noise signal when doing speech recognition."
	- collective outliers "Collective outliers can be subsets of novelties in data such as a signal that may indicate the discovery of new phenomena"
- How to fit hyper parameters?
	- Use validation set to determine [hyper parameters](http://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))?
	- Some of the most popular methods for outlier detection are
		- Z-Score or Extreme Value Analysis (parametric)
			- Some good ‘thumb-rule’ thresholds can be: 2.5, 3, 3.5 or more standard deviations.
		- Probabilistic and Statistical Modeling (parametric)
		- Linear Regression Models (PCA, LMS)
		- Proximity Based Models (non-parametric)
			- Dbscan (Density Based Spatial Clustering of Applications with Noise)
			- 
		- Information Theory Models
		- High Dimensional Outlier Detection Methods (high dimensional sparse data)!

# Constraints and data specifically for French project
- Constraint set
	- 6 or 7 constraints is upper limit of practical
		- *schwa
		- *CCC (recalculate)
		- *Max-schwa
		- *gn+C (reconsider?)
		- *[CC (reconsider?)
	- Sonority
		- two types of clusters?
		- Marie-Hélène Côté (2000)?
		- Aaron Kaplan (2015)?
	- Morphology (epenthesis vs. deletion)
		- look at subset of data, controlling for influence of morphology
- Data for French project (model input)
	- Frequency calculation
	- Two data sets?
		- Balanced <- type frequency
		- Realistically <- token frequency
- Split up original 
	- Clusters + sonority
	- Cluster vs. non-cluster
	- Controlling with cluster
	- Max
	- Input file
