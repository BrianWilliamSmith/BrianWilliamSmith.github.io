Title: French schwa project to do and notes
Author: Brian W. Smith
Date: 8/13 to present
Comment: French schwa project to do and notes
CSS: bws.css

***

<div class="title">

[%title]

</div>

{{TOC}}

===

# Model evaluation and goals

- How to evaluate and diagnose the model
	- What's the baseline model?
	- What are goals? (statistical evaluation)

# Learning algorithm and training

- The induction mechanism has two big parts, which I'll give names
	- **Outlier Detector** Detect whether a word is an outlier
	- **Constraint Choooser** Determine which constraint(s) to index the word to

- **Outlier Detector** possibilities
	- Residuals method (requires a model)
		- For every word, compare the model predictions to the real probabilities
		- Basically, calculate residuals
		- z-score residuals and find outliers with a threshhold (e.g. Z > 2.5 SD)
		- Nice thing: works with ANY model afaik, takes into account different categories and contexts
		- Bad thing: requires a ton of calculations
	- A dumb method: univariate tails cutoff
		+ Calculate a probability for every word
		+ If underlying probability isn't extreme (e.g. if schwa rates are closer to 50%), the probabilities will be normally distributed
		+ Read more on binomial distribution and central limit theorem to determine the actual conditions for the CLT (it's messy!)
		+ cut off words that are extreme -- these will be ones that are more categorical
		+ Nice thing: this doesn't require a model, is blindingly fast
		+ Bad thing: doesn't take into account context, will identify categorical cases as outliers, even if they obey the grammar
	- A weird adaptation of univariate tails cutoff
		+ Do the method above, but grouping together words with the same violation profile
		+ All words with the same violation profile should be normally distributed around their baseline probability
		+ Problem: will certainly break the CLT

- Read about outlier detection
	- ML outlier detection?
		- Lots about numerical data, hard to find proportion data
			- Weird, because this is a classifier! and ML always does classifiers
			- But: most ML outlier detection is supervised. Ugh.
		- We want ultivariate and unsupervised
	- Backward/forward model selection
		- [helpful slides on model selection: BIC vs. AIC vs. LRT](http://sct.uab.cat/estadistica/sites/sct.uab.cat.estadistica/files/uab_june_2017.pdf)
		- [overview of backward and forward selection](http://gerardnico.com/data_mining/stepwise_regression)
		- Stephanie Shih — backward model selection for exceptionality

# Outlier detection notes
- Link: [wiki article on Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance)
- Link: [towards data science article on outlier detection](http://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561)
- [Slides for paper discussing Mahalnobis distance](https://pdfs.semanticscholar.org/6712/7a1c5131218518225822067da0370bfc578f.pdf)
- Problem: we need to detect outliers in categorical data -- harder
	- Found this: [Detection of Outliers and Influential Observations in Binary Logistic Regression: An Empirical Study](https://scialert.net/fulltextmobile/?doi=jas.2011.26.35)
- Types of outliers
	- univariate vs. multivariate
		- "Univariate outliers can be found when looking at a distribution of values in a single feature space. Multivariate outliers can be found in a n-dimensional space (of n-features)."
	- point outliers "Point outliers are single data points that lay far from the rest of the distribution."
	- contextual outliers "Contextual outliers can be noise in data, such as punctuation symbols when realizing text analysis or background noise signal when doing speech recognition."
	- collective outliers "Collective outliers can be subsets of novelties in data such as a signal that may indicate the discovery of new phenomena"
- How to fit hyper parameters?
	- Use validation set to determine [hyper parameters](http://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))?
	- Some of the most popular methods for outlier detection are
		- Z-Score or Extreme Value Analysis (parametric)
			- Some good ‘thumb-rule’ thresholds can be: 2.5, 3, 3.5 or more standard deviations.
		- Probabilistic and Statistical Modeling (parametric)
		- Linear Regression Models (PCA, LMS)
		- Proximity Based Models (non-parametric)
			- Dbscan (Density Based Spatial Clustering of Applications with Noise)
			- 
		- Information Theory Models
		- High Dimensional Outlier Detection Methods (high dimensional sparse data)

# Constraints and data specifically for French project

- Constraint set
	- 6 or 7 constraints is upper limit of practical
		- *schwa
		- *CCC (recalculate)
		- *Max-schwa
		- *gn+C (reconsider?)
		- *[CC (reconsider?)
	- Sonority
		- two types of clusters?
		- Marie-Hélène Côté (2000)?
		- Aaron Kaplan (2015)?
	- Morphology (epenthesis vs. deletion)
		- look at subset of data, controlling for influence of morphology
- Data for French project (model input)
	- Frequency calculation
	- Two data sets?
		- Balanced <- type frequency
		- Realistically <- token frequency