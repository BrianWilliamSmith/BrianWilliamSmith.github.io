<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
	<meta charset="utf-8"/>
	<title>French schwa project to do and notes</title>
	<meta name="author" content="Brian W. Smith"/>
	<meta name="date" content="8/13 to present"/>
	<meta name="comment" content="French schwa project to do and notes"/>
	<link type="text/css" rel="stylesheet" href="bws.css"/>
</head>
<body>

<hr />

<div class="title">

<p>French schwa project to do and notes</p>

</div>

<div class="TOC">

<ul>
<li><a href="#modelevaluationandgoals">Model evaluation and goals</a></li>
<li><a href="#learningalgorithmandtraining">Learning algorithm and training</a></li>
<li><a href="#outlierdetectionnotes">Outlier detection notes</a></li>
<li><a href="#constraintsanddataspecificallyforfrenchproject">Constraints and data specifically for French project</a></li>
</ul>
</div>

<hr />

<h1 id="modelevaluationandgoals">Model evaluation and goals</h1>

<ul>
<li>How to evaluate and diagnose the model

<ul>
<li>What&#8217;s the baseline model?</li>
<li>What are goals? (statistical evaluation)</li>
</ul></li>
</ul>

<h1 id="learningalgorithmandtraining">Learning algorithm and training</h1>

<ul>
<li>The induction mechanism has two big parts, which I&#8217;ll give names

<ul>
<li><strong>Outlier Detector</strong> Detect whether a word is an outlier</li>
<li><strong>Constraint Choooser</strong> Determine which constraint(s) to index the word to</li>
</ul></li>
<li><strong>Outlier Detector</strong> possibilities

<ul>
<li>Residuals method (requires a model)

<ul>
<li>For every word, compare the model predictions to the real probabilities</li>
<li>Basically, calculate residuals</li>
<li>z-score residuals and find outliers with a threshhold (e.g. Z &gt; 2.5 SD)</li>
<li>Nice thing: works with ANY model afaik, takes into account different categories and contexts</li>
<li>Bad thing: requires a ton of calculations</li>
</ul></li>
<li>A dumb method: univariate tails cutoff

<ul>
<li>Calculate actual probability of schwa for every word</li>
<li>If the mean rate isn&#8217;t extreme (e.g. if schwa rates are closer to 50%), he probabilities will be normally distributed</li>
<li>Read more on binomial distribution and central limit theorem to determine the actual conditions for the CLT (it&#8217;s messy!)</li>
<li>cut off words that are extreme &#8211; these will be ones that are more categorical</li>
<li>Nice thing: this doesn&#8217;t require a model, is blindingly fast</li>
<li>Bad thing: doesn&#8217;t take into account context, will identify categorical cases as outliers, even if they obey the grammar</li>
</ul></li>
<li>A weird adaptation of univariate tails cutoff

<ul>
<li>Do the method above, but grouping together words with the same violation profile</li>
<li>Idea: same violation profile = same phonological and morphological environment

<ul>
<li>Depends on constraint set, of course</li>
<li>It&#8217;s more like: these are externally defined classes</li>
<li>for this data set, class is isomorphic with violation profile</li>
</ul></li>
<li>All words with the same violation profile should be normally distributed around some baseline probability</li>
<li>This actually works, I think, and is a great way to identify outliers</li>
<li>An open question is whether this should be done as part of outlier detection in the learner, or just done to identify outliers before the fact</li>
<li>This is very offline</li>
<li>Excitingly, constraint induction means the word is no longer part of the same class. it&#8217;s now in its own class &#8211; since it now violates a constraint it didn&#8217;t violate before</li>
<li>A weirdness: as outliers are removed, the mean and SD changes. this is a big problem in outlier removal geneally

<ul>
<li>mean is very sensitive to outliers, so the mean changes when they&#8217;re removed</li>
</ul></li>
<li>some words in Racine&#8217;s data simply don&#8217;t have schwa &#8211; these would need to be pruned ahead of time (idea: cut out words that don&#8217;t vary)</li>
<li>Normal approximation of the binomial distribution

<ul>
<li><a href="https://newonlinecourses.science.psu.edu/stat414/node/179/">psu</a></li>
<li><a href="http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Probability/BS704_Probability12.html">bu</a></li>
<li>requirements for n: np ≥ 5  and  n(1 − p) ≥ 5</li>
<li>If p(schwa) for some category is 0.1, we need at least 50 observations to approximate normal distribution

<ul>
<li>we need n(1&#8211;0.1)≥ 5, n ≥ (5/.9), n ≥ 5.5</li>
<li>we need n(0.1)≥ 5, n ≥ (5/.1), n ≥ 50</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>Read about outlier detection

<ul>
<li>ML outlier detection?

<ul>
<li>Lots about numerical data, hard to find proportion data

<ul>
<li>Weird, because this is a classifier! and ML always does classifiers</li>
<li>But: most ML outlier detection is supervised. Ugh.</li>
</ul></li>
<li>We want ultivariate and unsupervised</li>
</ul></li>
<li>Backward/forward model selection

<ul>
<li><a href="http://sct.uab.cat/estadistica/sites/sct.uab.cat.estadistica/files/uab_june_2017.pdf">helpful slides on model selection: BIC vs. AIC vs. LRT</a></li>
<li><a href="http://gerardnico.com/data_mining/stepwise_regression">overview of backward and forward selection</a></li>
<li>Stephanie Shih — backward model selection for exceptionality</li>
</ul></li>
</ul></li>
</ul>

<h1 id="outlierdetectionnotes">Outlier detection notes</h1>

<ul>
<li>Link: <a href="https://en.wikipedia.org/wiki/Mahalanobis_distance">wiki article on Mahalanobis distance</a></li>
<li>Link: <a href="http://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561">towards data science article on outlier detection</a></li>
<li><a href="https://pdfs.semanticscholar.org/6712/7a1c5131218518225822067da0370bfc578f.pdf">Slides for paper discussing Mahalnobis distance</a></li>
<li>Problem: we need to detect outliers in categorical data &#8211; harder

<ul>
<li>Found this: <a href="https://scialert.net/fulltextmobile/?doi=jas.2011.26.35">Detection of Outliers and Influential Observations in Binary Logistic Regression: An Empirical Study</a></li>
</ul></li>
<li>Types of outliers

<ul>
<li>univariate vs. multivariate

<ul>
<li>&#8220;Univariate outliers can be found when looking at a distribution of values in a single feature space. Multivariate outliers can be found in a n-dimensional space (of n-features).&#8221;</li>
</ul></li>
<li>point outliers &#8220;Point outliers are single data points that lay far from the rest of the distribution.&#8221;</li>
<li>contextual outliers &#8220;Contextual outliers can be noise in data, such as punctuation symbols when realizing text analysis or background noise signal when doing speech recognition.&#8221;</li>
<li>collective outliers &#8220;Collective outliers can be subsets of novelties in data such as a signal that may indicate the discovery of new phenomena&#8221;</li>
</ul></li>
<li>How to fit hyper parameters?

<ul>
<li>Use validation set to determine <a href="http://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">hyper parameters</a>?</li>
<li>Some of the most popular methods for outlier detection are

<ul>
<li>Z-Score or Extreme Value Analysis (parametric)

<ul>
<li>Some good ‘thumb-rule’ thresholds can be: 2.5, 3, 3.5 or more standard deviations.</li>
</ul></li>
<li>Probabilistic and Statistical Modeling (parametric)</li>
<li>Linear Regression Models (PCA, LMS)</li>
<li>Proximity Based Models (non-parametric)

<ul>
<li>Dbscan (Density Based Spatial Clustering of Applications with Noise)</li>
<li></li>
</ul></li>
<li>Information Theory Models</li>
<li>High Dimensional Outlier Detection Methods (high dimensional sparse data)!</li>
</ul></li>
</ul></li>
</ul>

<h1 id="constraintsanddataspecificallyforfrenchproject">Constraints and data specifically for French project</h1>

<ul>
<li>Constraint set

<ul>
<li>6 or 7 constraints is upper limit of practical

<ul>
<li>*schwa</li>
<li>*CCC (recalculate)</li>
<li>*Max-schwa</li>
<li>*gn+C (reconsider?)</li>
<li>*[CC (reconsider?)</li>
</ul></li>
<li>Sonority

<ul>
<li>two types of clusters?</li>
<li>Marie-Hélène Côté (2000)?</li>
<li>Aaron Kaplan (2015)?</li>
</ul></li>
<li>Morphology (epenthesis vs. deletion)

<ul>
<li>look at subset of data, controlling for influence of morphology</li>
</ul></li>
</ul></li>
<li>Data for French project (model input)

<ul>
<li>Frequency calculation</li>
<li>Two data sets?

<ul>
<li>Balanced &lt;- type frequency</li>
<li>Realistically &lt;- token frequency</li>
</ul></li>
</ul></li>
<li>Split up original

<ul>
<li>Clusters + sonority</li>
<li>Cluster vs. non-cluster</li>
<li>Controlling with cluster</li>
<li>Max</li>
<li>Input file</li>
</ul></li>
</ul>

</body>
</html>

